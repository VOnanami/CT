# train.py
from pathlib import Path
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim.lr_scheduler import _LRScheduler, ReduceLROnPlateau
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torch.amp import autocast, GradScaler
from tqdm import tqdm
from typing import Dict
from monai.metrics import SSIMMetric, PSNRMetric
import numpy as np
def get_lr(optimizer):
    for param_group in optimizer.param_groups:
        return param_group['lr']
@torch.no_grad()
def save_test_outputs(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
    output_dir: Path,
):
    """
    å¯¹æµ‹è¯•é›†è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œå¹¶å°†æ¨¡å‹çš„è¾“å‡ºç»“æœç›´æ¥ä¿å­˜ä¸º .npy æ–‡ä»¶ã€‚

    Args:
        model (nn.Module): è®­ç»ƒå¥½çš„æ¨¡å‹ã€‚
        loader (DataLoader): æµ‹è¯•é›†çš„DataLoaderã€‚
        device (torch.device): è®¡ç®—è®¾å¤‡ (cpu æˆ– cuda)ã€‚
        output_dir (Path): ä¿å­˜è¾“å‡º.npyæ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„ã€‚
    """
    model.eval()
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"æµ‹è¯•é›†è¾“å‡ºç»“æœå°†ä¿å­˜ä¸º .npy æ–‡ä»¶åˆ°: {output_dir}")

    pbar = tqdm(loader, desc="æ­£åœ¨ä¿å­˜æµ‹è¯•é›†è¾“å‡º (.npy)")

    for batch in pbar:
        input_images = batch["low_dose_img"].to(device)
        original_filenames = batch["filename"]

        with autocast(device_type=device.type if device.type == 'cuda' else 'cpu', enabled=(device.type == 'cuda')):
            reconstructed_images = model(input_images)

        reconstructed_images_np = reconstructed_images[-1].cpu().numpy()

        for i in range(reconstructed_images_np.shape[0]):
            # è·å–å•å¼ å›¾åƒçš„Numpyæ•°ç»„å’Œå¯¹åº”çš„æ–‡ä»¶å
            # æ•°ç»„å½¢çŠ¶é€šå¸¸æ˜¯ (C, H, W)ï¼Œä¾‹å¦‚ (1, 256, 256)
            img_np = reconstructed_images_np[i] 
            filename = original_filenames[i]
            
            # --- ä¸»è¦æ”¹åŠ¨åœ¨è¿™é‡Œ ---
            # 1. æ— éœ€è¿›è¡Œä»»ä½•åå¤„ç†ï¼Œç›´æ¥ä¿å­˜åŸå§‹çš„æµ®ç‚¹æ•°æ•°ç»„
            
            # 2. æ„å»ºæ–°çš„ä¿å­˜è·¯å¾„ï¼Œæ‰©å±•åä¸º .npy
            save_path = output_dir / f"{Path(filename).stem}_reconstructed.npy"
            
            # 3. ä½¿ç”¨ np.save ä¿å­˜æ•°ç»„
            np.save(save_path, img_np)
def train_epoch_swin(
    model: nn.Module,
    loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    device: torch.device,
    epoch: int,
    writer: SummaryWriter,
    scaler: GradScaler,
) -> None:
    model.train()
    pbar = tqdm(enumerate(loader), total=len(loader), desc=f"Epoch {epoch}")
    
    # å…³é”®ï¼šè®©æœ€ç»ˆè¾“å‡ºï¼ˆæœ€åä¸€ä¸ªå…ƒç´ ï¼‰æƒé‡æœ€å¤§ï¼Œä¸­é—´å±‚æƒé‡é€’å‡
    # å‡è®¾æ¨¡å‹è¾“å‡º4ä¸ªç»“æœï¼ˆå¯æ ¹æ®å®é™…è¾“å‡ºæ•°é‡è°ƒæ•´ï¼‰
    num_outputs = 4  # éœ€ä¸æ¨¡å‹å®é™…è¾“å‡ºæ•°é‡ä¸€è‡´
    # æƒé‡åˆ†é…ï¼šæœ€åä¸€ä¸ªè¾“å‡ºå æ¯” > 60%ï¼Œå‰é¢æŒ‰0.1, 0.2, 0.3...é€’å¢
    deep_sup_weights = [0.1, 0.2, 0.3, 0.4]  # æ€»å’Œä¸º1.0ï¼Œæœ€åä¸€ä¸ªæƒé‡æœ€å¤§
    # è‹¥è¾“å‡ºæ•°é‡ä¸åŒï¼Œä¾‹å¦‚3ä¸ªè¾“å‡ºï¼š[0.1, 0.2, 0.7]ï¼ˆæœ€åä¸€ä¸ªå 70%ï¼‰

    for step, x in pbar:
        noisy_images = x["low_dose_img"].to(device)
        target_images = x["high_dose_img"].to(device)

        optimizer.zero_grad(set_to_none=True)
        with autocast(device_type=device.type if device.type == 'cuda' else 'cpu', enabled=(device.type == 'cuda')):
            reconstructions = model(noisy_images)  # åˆ—è¡¨ï¼š[out1, out2, ..., outN]
            
            # è‡ªåŠ¨é€‚é…è¾“å‡ºæ•°é‡ï¼ˆæ¨èï¼æ— éœ€æ‰‹åŠ¨ä¿®æ”¹num_outputsï¼‰
            num_outputs = len(reconstructions)
            deep_sup_weights = [i / sum(range(1, num_outputs+1)) for i in range(1, num_outputs+1)]
            # ä¸Šé¢ä¸€è¡Œç­‰ä»·äºï¼šè¾“å‡ºnä¸ªæ—¶ï¼Œæƒé‡ä¸º[1,2,...,n]/(1+2+...+n)ï¼Œæœ€åä¸€ä¸ªæƒé‡æœ€å¤§
            # ä¾‹ï¼š4ä¸ªè¾“å‡º â†’ [1/10, 2/10, 3/10, 4/10]ï¼Œæœ€åä¸€ä¸ªå 40%
            
            total_loss = 0.0
            for out, weight in zip(reconstructions, deep_sup_weights):
                # ç¡®ä¿ä¸­é—´è¾“å‡ºä¸ç›®æ ‡å°ºå¯¸ä¸€è‡´ï¼ˆå¦‚éœ€ä¸Šé‡‡æ ·ï¼‰
                if out.shape[2:] != target_images.shape[2:]:
                    out = F.interpolate(
                        out, 
                        size=target_images.shape[2:], 
                        mode='bilinear' if target_images.ndim == 4 else 'trilinear',
                        align_corners=True
                    )
                loss_i = F.l1_loss(out.float(), target_images.float())
                total_loss += weight * loss_i

        scaler.scale(total_loss).backward()
        scaler.unscale_(optimizer)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        scaler.step(optimizer)
        scaler.update()

        if writer is not None:
            writer.add_scalar("lr", get_lr(optimizer), epoch * len(loader) + step)
            writer.add_scalar("train/loss_mae", total_loss.item(), epoch * len(loader) + step)
            # å¯é€‰ï¼šè®°å½•æœ€ç»ˆè¾“å‡ºçš„æŸå¤±ï¼ˆæ–¹ä¾¿è§‚å¯Ÿå…³é”®æŒ‡æ ‡ï¼‰
            final_loss = F.l1_loss(reconstructions[-1].float(), target_images.float())
            writer.add_scalar("train/final_output_loss", final_loss.item(), epoch * len(loader) + step)
        
        pbar.set_postfix(
            total_mae=f"{total_loss.item():.4f}",
            final_mae=f"{final_loss.item():.4f}",  # æ˜¾ç¤ºæœ€ç»ˆè¾“å‡ºçš„æŸå¤±
            lr=f"{get_lr(optimizer):.6f}"
        )

@torch.no_grad()
def eval_or_test(
    model: nn.Module,
    loader: DataLoader,
    device: torch.device,
    step: int = 0,
    writer: SummaryWriter = None,
    eval_mode: str = "val"
) -> Dict[str, float]:
    model.eval()
    
    total_mae = 0.0
    total_mse = 0.0
    total_psnr = 0.0
    total_ssim = 0.0
    
    pbar_desc = "Validating" if eval_mode == "val" else "Testing"
    pbar = tqdm(loader, desc=pbar_desc)

    for x in pbar:
        noisy_images = x["low_dose_img"].to(device)
        target_images = x["high_dose_img"].to(device)

        with autocast(device_type=device.type if device.type == 'cuda' else 'cpu', enabled=(device.type == 'cuda')):
            reconstruction = model(noisy_images)
            reconstruction = reconstruction[-1]
            # --- æ ¸å¿ƒä¿®æ”¹åœ¨è¿™é‡Œ ---
            # 1. åŠ¨æ€è®¡ç®—å½“å‰æ‰¹æ¬¡ç›®æ ‡å›¾åƒçš„åŠ¨æ€èŒƒå›´
            data_range = target_images.max() - target_images.min()
            
            # 2. å®‰å…¨æ£€æŸ¥ï¼šå¦‚æœ data_range å‡ ä¹ä¸º0 (å›¾åƒæ˜¯å¹³å¦çš„)ï¼Œåˆ™è¯¥æ‰¹æ¬¡çš„PSNR/SSIMæ— æ„ä¹‰
            if data_range < 1e-6:
                # å¯¹äºè¿™ä¸ªç‰¹æ®Šçš„æ‰¹æ¬¡ï¼Œæˆ‘ä»¬è·³è¿‡PSNR/SSIMè®¡ç®—ï¼Œæˆ–è€…ç»™ä¸€ä¸ªåˆç†çš„å€¼
                # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°è®¤ä¸ºè¿™ä¸ªæ‰¹æ¬¡çš„PSNR/SSIMè´¡çŒ®ä¸º0
                psnr = torch.tensor(0.0, device=device)
                ssim = torch.tensor(0.0, device=device)
            else:
                # åªæœ‰åœ¨data_rangeæœ‰æ•ˆæ—¶æ‰è¿›è¡Œè®¡ç®—
                psnr_metric = PSNRMetric(max_val=data_range.item())
                ssim_metric = SSIMMetric(data_range=data_range, spatial_dims=2)
                psnr = psnr_metric(reconstruction, target_images)
                ssim = ssim_metric(reconstruction, target_images)
            
            # MAE å’Œ MSE çš„è®¡ç®—ä¸å—å½±å“
            mae = F.l1_loss(reconstruction, target_images)
            mse = F.mse_loss(reconstruction, target_images)
            # --- ç»“æŸä¿®æ”¹ ---
            
        # ç´¯åŠ æ¯ä¸ªæ‰¹æ¬¡çš„ç»“æœ
        batch_size = noisy_images.shape[0]
        total_mae += mae.item() * batch_size
        total_mse += mse.item() * batch_size
        total_psnr += psnr.mean().item() * batch_size
        total_ssim += ssim.mean().item() * batch_size

    # è®¡ç®—æœ€ç»ˆçš„å¹³å‡å€¼
    n_samples = len(loader.dataset)
    avg_mae = total_mae / n_samples
    avg_mse = total_mse / n_samples
    avg_psnr = total_psnr / n_samples
    avg_ssim = total_ssim / n_samples

    if writer is not None:
        writer.add_scalar(f"{eval_mode}/mae", avg_mae, step)
        writer.add_scalar(f"{eval_mode}/mse", avg_mse, step)
        writer.add_scalar(f"{eval_mode}/psnr", avg_psnr, step)
        writer.add_scalar(f"{eval_mode}/ssim", avg_ssim, step)

    return {
        "mae": avg_mae, "mse": avg_mse, "psnr": avg_psnr, "ssim": avg_ssim
    }
def train_swin(
    model: nn.Module,
    start_epoch: int,
    best_mae_loss: float,
    train_loader: DataLoader,
    val_loader: DataLoader,     # <--- æˆ‘ä»¬å°†ä»è¿™é‡Œè·å–éªŒè¯é›†
    test_loader: DataLoader,
    optimizer: torch.optim.Optimizer,
    n_epochs: int,
    eval_freq: int,
    scheduler: _LRScheduler,
    writer_train: SummaryWriter,
    writer_val: SummaryWriter,
    device: torch.device,
    run_dir: Path,
) -> float:
    scaler = GradScaler(enabled=(device.type == 'cuda'))
    raw_model = model.module if hasattr(model, "module") else model

    # ==============================================================================
    # --- æ–°å¢ï¼šåœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œå¯¹éªŒè¯é›†è¿›è¡Œä¸€æ¬¡æ€§çš„å¥åº·æ‰«æ ---
    # ==============================================================================
    print("\n--- [å¯åŠ¨æ£€æŸ¥] æ­£åœ¨æ‰«æéªŒè¯é›†ä¸­çš„æ½œåœ¨é—®é¢˜æ•°æ® ---")
    
    # ä» DataLoader ä¸­è·å–å…¶åº•å±‚çš„ Dataset å¯¹è±¡
    val_ds = val_loader.dataset 
    
    problematic_files_info = []
    # ä½¿ç”¨ tqdm åˆ›å»ºè¿›åº¦æ¡
    for i in tqdm(range(len(val_ds)), desc="æ‰«æéªŒè¯é›†"):
        try:
            # val_ds[i] ä¼šè‡ªåŠ¨åº”ç”¨ val_test_transforms
            sample = val_ds[i]
            
            # æ£€æŸ¥ sample ä¸­çš„æ¯ä¸€ä¸ªå¼ é‡
            for key, tensor in sample.items():
                if isinstance(tensor, torch.Tensor): # åªæ£€æŸ¥å¼ é‡
                    if torch.isnan(tensor).any() or torch.isinf(tensor).any():
                        # ä» val_ds.data è®¿é—®åŸå§‹çš„æ–‡ä»¶è·¯å¾„å­—å…¸
                        problem_info = val_ds.data[i]
                        problematic_files_info.append(problem_info)
                        # æ‰¾åˆ°ä¸€ä¸ªé—®é¢˜å°±è¶³å¤Ÿäº†ï¼Œè·³å‡ºå†…å±‚å¾ªç¯
                        break 
        except Exception as e:
            problem_info = val_ds.data[i]
            print(f"\nå¤„ç† Index {i} æ—¶é‡åˆ°åŠ è½½æˆ–å˜æ¢é”™è¯¯: {e}, æ–‡ä»¶ä¿¡æ¯: {problem_info}")
            problematic_files_info.append(problem_info)

    if problematic_files_info:
        print("\n\n" + "="*80)
        print(f"ğŸš¨ [å¯åŠ¨å¤±è´¥] æ£€æŸ¥åˆ° {len(problematic_files_info)} ç»„é—®é¢˜æ•°æ®ï¼Œè®­ç»ƒå·²åœæ­¢ã€‚")
        print("è¯·ä»æ‚¨çš„CSVæ–‡ä»¶ä¸­ç§»é™¤ä»¥ä¸‹è¿™äº›æ•°æ®å¯¹åº”çš„è¡Œï¼Œç„¶åé‡æ–°å¼€å§‹è®­ç»ƒï¼š")
        print("="*80)
        for info in problematic_files_info:
            print(info)
        print("="*80)
        # æŠ›å‡ºå¼‚å¸¸ï¼Œä¸­æ–­æ•´ä¸ªç¨‹åº
        raise ValueError("éªŒè¯é›†ä¸­å‘ç°æŸåæˆ–æ— æ³•å¤„ç†çš„æ•°æ®ï¼Œè¯·æ¸…ç†åé‡è¯•ã€‚")
    else:
        print("âœ… [å¯åŠ¨æˆåŠŸ] æ•°æ®é›†å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°é—®é¢˜ã€‚å¼€å§‹æ­£å¼è®­ç»ƒ...")
    # --- æ‰«æé€»è¾‘ç»“æŸ ---


    # --- åŸæœ‰çš„è®­ç»ƒå¾ªç¯ä¿æŒä¸å˜ ---
    for epoch in range(start_epoch, n_epochs):
        train_epoch_swin(
            model=model, loader=train_loader, optimizer=optimizer,
            device=device, epoch=epoch, writer=writer_train, scaler=scaler,
        )

        if (epoch + 1) % eval_freq == 0:
            val_metrics = eval_or_test(
                model=model, loader=val_loader, device=device,
                step=len(train_loader) * (epoch + 1), writer=writer_val, eval_mode="val"
            )
            val_mae = val_metrics["mae"]
            
            # å¦‚æœéªŒè¯è¿‡ç¨‹ä¸­ä»ç„¶å‡ºç°NaNï¼Œè¯´æ˜é—®é¢˜å¯èƒ½æ¥è‡ªæ¨¡å‹å†…éƒ¨ï¼ˆå¦‚BatchNormï¼‰
            if torch.isnan(torch.tensor(val_mae)):
                print(f"Epoch {epoch + 1} | éªŒè¯æ—¶å‡ºç°NaNï¼è¿™å¯èƒ½æ˜¯ä¸€ä¸ªæ¨¡å‹å†…éƒ¨é—®é¢˜ï¼ˆå¦‚BatchNormï¼‰ã€‚è®­ç»ƒåœæ­¢ã€‚")
                # å¯ä»¥åœ¨è¿™é‡Œæå‰ç»“æŸè®­ç»ƒ
                break

            print(
                f"Epoch {epoch + 1} | Val MAE: {val_mae:.4f} | Val MSE: {val_metrics['mse']:.4f} | "
                f"Val PSNR: {val_metrics['psnr']:.2f} dB | Val SSIM: {val_metrics['ssim']:.4f}"
            )
            
            # ... (åç»­çš„ä¿å­˜æ¨¡å‹ã€å­¦ä¹ ç‡è°ƒåº¦ç­‰é€»è¾‘å®Œå…¨ä¿æŒä¸å˜) ...
            if val_mae < best_mae_loss:
                print(f"âœ… New best val MAE: {val_mae:.4f}. Saving model...")
                best_mae_loss = val_mae
                torch.save(raw_model.state_dict(), run_dir / "best_model.pth")
            
            checkpoint = {
                "epoch": epoch + 1,
                "state_dict": raw_model.state_dict(),
                "optimizer": optimizer.state_dict(),
                "scheduler": scheduler.state_dict(),
                "best_loss": best_mae_loss,
            }
            torch.save(checkpoint, run_dir / "checkpoint.pth")

            if isinstance(scheduler, ReduceLROnPlateau):
                scheduler.step(val_mae) 
            else:
                scheduler.step()

    # ... (è®­ç»ƒç»“æŸåçš„æœ€ç»ˆæµ‹è¯•å’Œä¿å­˜é€»è¾‘ä¹Ÿå®Œå…¨ä¿æŒä¸å˜) ...
    print("ğŸ Training finished!")
    print("Saving final model...")
    torch.save(raw_model.state_dict(), run_dir / "final_model.pth")

    print("\n--- Running Final Evaluation on Test Set ---")
    best_model_path = run_dir / "best_model.pth"
    if best_model_path.exists():
        print(f"Loading best model from {best_model_path}")
        raw_model.load_state_dict(torch.load(best_model_path, map_location=device))
    else:
        print("No best model found, using final model for testing.")

    test_metrics = eval_or_test(model=model, loader=test_loader, device=device, eval_mode="test")
    print("\n--- Test Set Results ---")
    print(f"  PSNR: {test_metrics['psnr']:.2f} dB")
    print(f"  SSIM: {test_metrics['ssim']:.4f}")
    print(f"  MAE:  {test_metrics['mae']:.4f}")
    print(f"  MSE:  {test_metrics['mse']:.4f}")
    print("------------------------\n")
    output_save_dir = run_dir / "test_outputs"
    
    # è°ƒç”¨å‡½æ•°ï¼Œä¼ å…¥æœ€å¥½çš„æ¨¡å‹ã€æµ‹è¯•é›†åŠ è½½å™¨ã€è®¾å¤‡å’Œè¾“å‡ºè·¯å¾„
    save_test_outputs(
        model=model, # ä½¿ç”¨å·²åŠ è½½äº†æœ€ä½³æƒé‡çš„æ¨¡å‹
        loader=test_loader,
        device=device,
        output_dir=output_save_dir
    )
    print("âœ… All test outputs have been saved.")
    return best_mae_loss
